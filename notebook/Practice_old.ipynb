{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "## 1. [Introduction](#s1)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#s6) - Kashchikhin\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#s7) - Kashchikhin\n",
    "\n",
    "## 4. [OpenVINO(TM) API](#s7) – Tugaryov\n",
    "\n",
    "Object Detection sample: http://127.0.0.1:5665/jupyter/lab/tree/tutorials/object_detection_ssd/tutorial_object_detection_ssd.ipynb\n",
    "W/o downloader and w/updated cells\n",
    "\n",
    "## 5. [Practice](#s15) – Tugaryov\n",
    "\n",
    "Task 1: apply pre-defined blur method to given image at inferred coordinates (photo with several faces)\n",
    "\n",
    "Task 2: add blurring logic to pre-defined video processor\n",
    "\n",
    "Task 3: replace each face on the photo with a smile with corresponding emotion\n",
    "\n",
    "Task 4: emotional smile on the video\n",
    "\n",
    "** Task 5: Telegram Bot - Smiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "## Example #1\n",
    "\n",
    "![](pictures/deep-learning.png)\n",
    "\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)\n",
    "\n",
    "## Example #2\n",
    "\n",
    "![](pictures/use-cases.png)\n",
    "\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.com/content/www/us/en/artificial-intelligence/posts/difference-between-ai-machine-learning-deep-learning.html)\n",
    "\n",
    "## Example #3\n",
    "\n",
    "![](pictures/system.png)\n",
    "\n",
    "Author: Mark Robins, Intel, [Link](https://www.intel.ru/content/www/ru/ru/artificial-intelligence/posts/deep-learning-training-and-inference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "1. Neural Network\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and a non-linear activation function.\n",
    "\n",
    "2. Inference\n",
    "\n",
    "Process of neural network execution: feeding data to the network and getting the results. \n",
    "\n",
    "3. Dataset\n",
    "\n",
    "A dataset is a collection of data that can be treated by a neural network as a single unit for analytic and prediction purposes.\n",
    "\n",
    "4. Optimization\n",
    "\n",
    "To accelerate the inference of deep learning models by applying special methods without model retraining or fine-tuning, like post-training quantization.\n",
    "The process of transforming the models that were trained in the floating-point precision into the models with integer representation with floating/fixed-point quantization operations between the layers.\n",
    "\n",
    "5. Accuracy (?)\n",
    "\n",
    "Measure for how good or bad a neural network solves its task. Accuracy could be represented by different metrics depending on the task. \n",
    "\n",
    "6. Deployment (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenVINO™ toolkit is a comprehensive toolkit for quickly developing applications and solutions that solve a variety of tasks including emulation of human vision, automatic speech recognition, natural language processing, recommendation systems, and many others. Based on latest generations of artificial neural networks, including Convolutional Neural Networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned and optimized OpenVINO™ models\n",
    "### For various tasks\n",
    "Select a **pretrained** model or models suitable for your needs from the Open Model Zoo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO™ Deep Learning Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Workbench (DL Workbench) is an official OpenVINO™ graphical interface designed to make the production of pretrained deep learning models significantly easier.\n",
    "\n",
    "DL Workbench combines OpenVINO™ tools to assist you with the most commonly used tasks: import a model, analyze its performance and accuracy, visualize the outputs, optimize and prepare the model for deployment in a matter of minutes. DL Workbench will take you through the full OpenVINO™ workflow, providing the opportunity to learn about various toolkit components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit-dl-wb-highlighted.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Workbench capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_dl_wb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Workbench Workflow\n",
    "1. Select a model\n",
    "    * Import a model\n",
    "    * Download from Open Model Zoo\n",
    "2. Create a project\n",
    "    * Select a target environment\n",
    "    * Select or create a dataset\n",
    "    * Run inference\n",
    "3. Optimize the model\n",
    "    * Apply INT8 calibration\n",
    "        * If performance is more important than accuracy: Default method\n",
    "        * If accuracy is of paramount importance: Accuracy Aware method\n",
    "4. Assess the quality\n",
    "    * Analyze throughput and latency\n",
    "    * Measure accuracy\n",
    "    * Visualize model output\n",
    "    * etc.\n",
    "5. Find an optimal inference configuration (profile the model)\n",
    "    * Find an optimal batch and stream combination\n",
    "6. Prepare to deploy\n",
    "    * Create a deployment package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL WB Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Select/Import a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Open Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/import_model_wb_omz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload a local model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/upload-local-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import or create a dataset\n",
    "\n",
    "We have created a custom dataset which you can use during this workshop.\n",
    "\n",
    "**TODO**: insert link.\n",
    "\n",
    "1. Execute the following cell to download an archive with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O dataset.zip LINK_TO_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. You should see a `dataset.zip` on the left in the file tree;\n",
    "\n",
    "3. Download it on your machine (right-click + `Download`);\n",
    "4. Unarchive it;\n",
    "5. Go to DL Workbench;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/validation_dataset_import.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Drag & drop the images from the archive to create a not-annotated dataset.\n",
    "\n",
    "![](pictures/custom_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a project (Run inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/create_project_selected.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/dashboard-page.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/analyze.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INT8 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/calibration-int8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/dashboard-parent-vs-optimized.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Profile the model\n",
    "\n",
    "To find the best parameters for the inference.\n",
    "\n",
    "NOTE: Both original and optimized models could be profiled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/group_inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/group_inference_results_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL WB Workflow recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/DL_WB_workflow.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App - Face Replacer\n",
    "Face detection, Emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (INSERT) Picture before - picture after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "#### Part 0 - Obtain a model\n",
    "0. Go to DL WB\n",
    "1. Find a suitable face-detection model \n",
    "2. Experiment with it, optimize, assess results\n",
    "3. Export\\download the model\n",
    "\n",
    "#### Part 1 - OpenVINO Python API + minimal app\n",
    "\n",
    "Show tutorial_object_detection for OV Python API\n",
    "\n",
    "4. Prerequisites\n",
    "    * copy the model path from DL WB\n",
    "    * sample data (video) is placed in the folder with this notebook\n",
    "5. OpenVINO Python API for work with neural networks\n",
    "6. Image/video pre-processing with OpenCV\n",
    "7. Neural network execution - Inference\n",
    "8. Results processing\n",
    "    * Describe the model and its output so that it is understandable how to post-process\n",
    "9. Have a video with faces replaced\n",
    "\n",
    "#### Part 2 - Enriching/Building upon the app / Adding new functionality\n",
    "\n",
    "9. Prepare another neural network\n",
    "10. Integrate new network in the app\n",
    "\n",
    "OR - give a choice of either following the presenter with deployment of continuing with the emotion recognition\n",
    "\n",
    "#### Part 3 - Deploy the app\n",
    "11. Prepare deployment package\\bundle with model and download it\n",
    "    * Ubuntu - go to DL WB\n",
    "    * non-Ubuntu - supply with os-specific bundles\n",
    "12. Prepare platform\n",
    "    * Copy/download the necessary assets (OpenVINO deployment package, model)\n",
    "    * Prepare environment using setupvars\n",
    "13. Prepare sample\\application and a Telegram bot\n",
    "    * Clone the repository with the template\n",
    "    * Copy your code from the notebook and integrate in the template\n",
    "13. Deploy\n",
    "14. Enjoy\n",
    "\n",
    "\n",
    "#### Demo of the completed bot\\application in case of the out of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO™ API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open Visual Inference & Neural Network Optimization (OpenVINO™) toolkit](https://software.intel.com/openvino-toolkit). This tutorial will go step-by-step through the necessary steps to demonstrate object detection on images. Object detection is performed using a pre-trained network and running it using the Intel® Distribution of OpenVINO™ toolkit Inference Engine.\n",
    "\n",
    "Object Detection in Computer Vision is a task of finding objects and locating them in the image.\n",
    "\n",
    "The tutorial guides you through the following steps:\n",
    "\n",
    "1. [Import required modules](#1.-Import-Required-Modules) \n",
    "3. [Configure inference: path to a model and other data](#3.-Configure-an-Inference)\n",
    "4. [Initialize the OpenVINO™ runtime](#4.-Initialize-the-OpenVINO™-Runtime)\n",
    "5. [Read the model](#5.-Read-the-Model)\n",
    "6. [Make the model executable](#6.-Make-the-Model-Executable)\n",
    "7. [Prepare an image for model inference](#7.-Prepare-an-Image-for-Model-Inference)\n",
    "8. [Infer the model](#8.-Infer-the-Model)\n",
    "9. [Show predictions](#9.-Show-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Modules\n",
    "\n",
    "Import the Python* modules that you will use in the sample code:\n",
    "- [pathlib](https://docs.python.org/3/library/os.html#module-os) is a standard Python module used for filename parsing.\n",
    "- [cv2](https://docs.opencv.org/trunk/) is an OpenCV module used to work with images.\n",
    "- [NumPy](http://www.numpy.org/) is an array manipulation module used to process images as arrays.\n",
    "- [OpenVINO Inference Engine](https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) is an OpenVINO™ Python API module used for inference.\n",
    "- [IPython](https://ipython.readthedocs.io/en/stable/index.html) is an IPython API uused for showing images and videos in the notebook\n",
    "\n",
    "Run the cell below to import the modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "from IPython.display import HTML, Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure an Inference\n",
    "\n",
    "Once you have the OpenVINO™ IR of your model, you can start experimenting with it by inferring it and inspecting its output. \n",
    "\n",
    "> **NOTE**: Copy the paths to the `.xml` and `.bin` files from the DL Workbench UI and paste them below.\n",
    "#### Required parameters\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**model_xml**| Path to the `.xml` file of OpenVINO™ IR of your model\n",
    "**model_bin**| Path to the `.bin` file of OpenVINO™ IR of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "face_detection_model_xml = 'data/models/face-detection-adas-0001.xml'\n",
    "face_detection_model_bin = 'data/models/face-detection-adas-0001.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Parameters\n",
    "\n",
    "Experiment with optional parameters after you go the full workflow of the tutorial.\n",
    "\n",
    "Parameter| Explanation\n",
    "---|---\n",
    "**input_image_path**| Path to an input image. Use the `car.bmp` image placed in the directory of the notebook or, if you have imported a dataset in the DL Workbench, copy the path to an image in the dataset.\n",
    "**device**| Specify the [target device](https://docs.openvinotoolkit.org/latest/workbench_docs_Workbench_DG_Select_Environment.html) to infer on: CPU, GPU, or MYRIAD. Note that the device must be present. For this tutorial, use `CPU` which is known to be present.\n",
    "**prob_threshold**| Probability threshold to filter detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image file. \n",
    "input_image_path = 'data/input_image.JPG'\n",
    "\n",
    "# Input video file\n",
    "input_video_path = 'data/input.mp4'\n",
    "\n",
    "# Output video file\n",
    "output_video_path = 'data/output.mp4'\n",
    "\n",
    "# Device to use\n",
    "device = 'CPU'\n",
    "\n",
    "# Minimum percentage threshold to detect an object\n",
    "prob_threshold = 50\n",
    "\n",
    "print(\n",
    "f'''Configuration parameters settings:\n",
    "    model_xml={face_detection_model_xml},\n",
    "    model_bin={face_detection_model_bin},\n",
    "    input_image_path={input_image_path},\n",
    "    device={device}, \n",
    "    prob_threshold={prob_threshold}''',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialize the OpenVINO™ Runtime\n",
    "\n",
    "Once you define the parameters, let's initiate the `IECore` object that accesses OpenVINO™ runtime capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Inference Engine instance\n",
    "ie_core = IECore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Read the Model\n",
    "\n",
    "Put the IR of your model in the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the network from IR files\n",
    "face_detection_network = ie_core.read_network(model=face_detection_model_xml, weights=face_detection_model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make the Model Executable\n",
    "\n",
    "Reading a network is not enough to start a model inference. The model must be loaded to a particular abstraction representing a particular accelerator. In OpenVINO™, this abstraction is called *plugin*. A network loaded to a plugin becomes executable and will be inferred in one of the next steps. \n",
    "\n",
    "After loading, we keep necessary model information such as names of input and output blobs: `input_blob` and `output_blob`. Let's remember the input dimensions of your model:\n",
    "- `n` - input batch size\n",
    "- `c` - number of input channels. Often, it is `1` or `3`, which means that the model expects either a grayscale or a color image.\n",
    "- `h` - input image height\n",
    "- `w` - input image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_executable_network = ie_core.load_network(network=face_detection_network, device_name=device)\n",
    "\n",
    "# Store names of input and output blobs\n",
    "face_detection_input_blob = next(iter(face_detection_network.input_info))\n",
    "face_detection_output_blob = next(iter(face_detection_network.outputs))\n",
    "\n",
    "# Read the input dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "face_detection_n, face_detection_c, face_detection_h, face_detection_w = face_detection_network.input_info[face_detection_input_blob].input_data.shape\n",
    "print(f'Loaded the model into the Inference Engine for the {device} device.'), \n",
    "print(f'Face Detection model input dimensions: n={face_detection_n}, c={face_detection_c}, h={face_detection_h}, w={face_detection_w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Prepare an Image for Model Inference\n",
    "\n",
    "Now let's read and prepare the input image by resizing and re-arranging its dimensions according to the input dimensions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to load the input image\n",
    "def load_input_image(input_path):   \n",
    "    # Use OpenCV to load the input image\n",
    "    image = cv2.imread(input_path)\n",
    "    return image\n",
    "\n",
    "# Define the function to pre-process the input image\n",
    "def pre_process_input_image(image, n, c, h, w):\n",
    "    # Resize the image dimensions from image to model input w x h\n",
    "    in_frame = cv2.resize(image, (w, h))\n",
    "    # Change data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    # Reshape to input dimensions\n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    return in_frame\n",
    "\n",
    "def show_images(image: np.ndarray):\n",
    "    _, data = cv2.imencode('.jpg', image) \n",
    "    image = Image(data=data)\n",
    "    display(image)\n",
    "\n",
    "# Use OpenCV to load the input image\n",
    "original_image = cv2.imread(input_image_path)\n",
    "original_image_h, original_image_w, *_ = original_image.shape\n",
    "\n",
    "# Resize the input image\n",
    "input_frame = pre_process_input_image(original_image, face_detection_n, face_detection_c, face_detection_h, face_detection_w)\n",
    "\n",
    "# Display the input image\n",
    "show_images(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detection_executable_network.infer(\n",
    "    inputs={\n",
    "        face_detection_input_blob: input_frame\n",
    "    }\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Show Predictions\n",
    "\n",
    "The next step is to parse the inference results and draw boxes over the objects detected in the image.\n",
    "\n",
    "A result of model inference (`res`) is an array of predictions. Each prediction `obj` has a following structure:\n",
    "\n",
    "- `obj[1]`: class ID, or the type of a detected object\n",
    "- `obj[2]`: Confidence level that currently detected object is an instance of the predicted class\n",
    "- `obj[3]`: lower x coordinate of the detected object \n",
    "- `obj[4]`: lower y coordinate of the detected object\n",
    "- `obj[5]`: upper x coordinate of the detected object\n",
    "- `obj[6]`: upper y coordinate of the detected object\n",
    "\n",
    "For each detected object, the output from the model will include an integer to indicate which type of the object, such as car or human, has been detected. To translate the integer into a more readable text string, use a label mapping file. The label mapping file is a text file of the format `n: string` (for example, `7: car`) that is loaded into a lookup table to be used later when labeling detected objects.\n",
    "\n",
    "Now we have an image where every detected object is bounded with a box with class id and confidence level. To replace class ids with their names, you need a label mapping file. You can find the sample label mapping file in the current directory with the name `labels.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process inference results\n",
    "def process_face_detection_results(original_image, results):\n",
    "    processed_image = original_image\n",
    "    # Get output results\n",
    "    result = results[face_detection_output_blob]\n",
    "    color = (12.5, 255, 255)\n",
    "    original_input_h, original_input_w, *_ = original_image.shape\n",
    "    \n",
    "        \n",
    "    # Loop through all possible results\n",
    "    for face in result[0][0]:\n",
    "        probability = round(face[2] * 100, 1)\n",
    "        \n",
    "        # If probability is more than the specified threshold, draw and label the box \n",
    "        if probability > prob_threshold:\n",
    "            # Get coordinates of the box containing the detected object\n",
    "            xmin = int(face[3] * original_input_w)\n",
    "            ymin = int(face[4] * original_input_h)\n",
    "            xmax = int(face[5] * original_input_w)\n",
    "            ymax = int(face[6] * original_input_h)\n",
    "\n",
    "            # Draw the box and label for the detected object\n",
    "            cv2.rectangle(processed_image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "            cv2.putText(processed_image, f'{probability} %', (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 1, color, 2)\n",
    "    return processed_image\n",
    "\n",
    "processed_image = process_face_detection_results(original_image, face_detection_inference_results)\n",
    "\n",
    "show_images(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: apply pre-defined blur method to given image at inferred coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_region(image: np.ndarray) -> np.ndarray:\n",
    "    height, width = image.shape[:2]\n",
    "    pixels_count = 16\n",
    "    temp = cv2.resize(image, (pixels_count, pixels_count), interpolation=cv2.INTER_LINEAR)\n",
    "    return cv2.resize(temp, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_input_h, original_input_w, *_ = processed_image.shape\n",
    "\n",
    "processed_image = original_image\n",
    "\n",
    "face_detection_inference_result = face_detection_inference_results[face_detection_output_blob]\n",
    "\n",
    "for detected_face in face_detection_inference_result[0][0]:\n",
    "    confidence = round(detected_face[2] * 100, 1)\n",
    "        \n",
    "    # If confidence is more than the specified threshold, draw and label the box \n",
    "    if confidence > prob_threshold:\n",
    "        \n",
    "        # Get coordinates of the box containing the detected object\n",
    "        xmin = int(detected_face[3] * input_w)\n",
    "        ymin = int(detected_face[4] * input_h)\n",
    "        xmax = int(detected_face[5] * input_w)\n",
    "        ymax = int(detected_face[6] * input_h)\n",
    "\n",
    "        face = original_image[ymin:ymax, xmin:xmax]\n",
    "        processed_image[ymin:ymax, xmin:xmax] = blur_region(face)\n",
    "\n",
    "show_images(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: add blurring logic to pre-defined video processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "input_frame_width = int(input_video_stream.get(3))   # float `width`\n",
    "input_frame_height = int(input_video_stream.get(4))  # float `height`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prapare_out_video_stream(input_video_stream: cv2.VideoCapture, output_video_file_path: str) -> cv2.VideoWriter:\n",
    "    width  = int(input_video_stream.get(3))\n",
    "    height = int(input_video_stream.get(4))\n",
    "    video_writer = cv2.VideoWriter(output_video_file_path, cv2.VideoWriter_fourcc(*'avc1'), 20, (width, height))\n",
    "    return video_writer\n",
    "\n",
    "output_video_stream = prapare_out_video_stream(input_video_stream, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    # Read the next frame from the intput video \n",
    "    finish, original_frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not finish:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    \n",
    "    # Prepare frame for inference\n",
    "    in_frame = pre_process_input_image(original_frame, face_detection_n, face_detection_c, face_detection_h, face_detection_w )\n",
    "    \n",
    "    \n",
    "    face_detection_inference_results = face_detection_executable_network.infer(inputs={face_detection_input_blob: in_frame})  \n",
    "    \n",
    "    inference_result = face_detection_inference_results[face_detection_output_blob]\n",
    "\n",
    "    for detected_face in inference_result[0][0]:\n",
    "        probability = round(detected_face[2] * 100, 1)\n",
    "\n",
    "        # If probability is more than the specified threshold, draw and label the box \n",
    "        if probability > prob_threshold:\n",
    "\n",
    "            # Get coordinates of the box containing the detected object\n",
    "            xmin = int(detected_face[3] * input_frame_width)\n",
    "            ymin = int(detected_face[4] * input_frame_height)\n",
    "            xmax = int(detected_face[5] * input_frame_width)\n",
    "            ymax = int(detected_face[6] * input_frame_height)\n",
    "\n",
    "            face = original_frame[ymin:ymax, xmin:xmax]\n",
    "            original_frame[ymin:ymax, xmin:xmax] = blur_region(face)\n",
    "    \n",
    "    # Write the resulting frame to the output stream\n",
    "    output_video_stream.write(original_frame)\n",
    "    \n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{output_video_path}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: replace each face on the photo with a smile with corresponding emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the next step? Often from neural networks build pipelines. It is to use the results of the first neural network as an input for the next neural network. \n",
    "Let's try to build a pipeline from two networks:  first is finds a person on the video and the next to recognize the emotions of this person\n",
    "\n",
    "We have already run the first network. And find the person on the video.\n",
    "The next step is to find a network for emotion recognition.\n",
    "There is a good neural network in the [OpenModelZOO](https://docs.openvinotoolkit.org/latest/omz_models_group_intel.html) - [emotions-recognition-retail-0003 network](https://docs.openvinotoolkit.org/latest/omz_models_model_emotions_recognition_retail_0003.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download emotions-recognition-retail-0003 network\n",
    "Run the Model Downloader eith needed arguments to download the emotions-recognition-retail-0003 network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ~/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name emotions-recognition-retail-0003 --precision FP16 --output_dir data/model\n",
    "!mv data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.* data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IR files\n",
    "emotion_recognition_model_xml = 'data/models/emotions-recognition-retail-0003.xml'\n",
    "emotion_recognition_model_bin = 'data/models/emotions-recognition-retail-0003.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_network = ie_core.read_network(emotion_recognition_model_xml, emotion_recognition_model_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the network to a device\n",
    "\n",
    "Use the instance of `IECore`.\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_network_loaded_on_device = ie_core.load_network(emotion_recognition_network, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Open the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "input_frame_width = int(input_video_stream.get(3))   # float `width`\n",
    "input_frame_height = int(input_video_stream.get(4))  # float `height`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create an output video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_stream = prapare_out_video_stream(input_video_stream, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_input_layer = next(iter(emotion_recognition_network.input_info))\n",
    "emotion_recognition_input_blob = emotion_recognition_network.input_info[emotion_recognition_input_layer].input_data\n",
    "\n",
    "print(f'Input layer of the emotions-recognition-retail-0003 is {emotion_recognition_input_layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_n, emotion_recognition_c, emotion_recognition_h, emotion_recognition_w = emotion_recognition_input_blob.shape\n",
    "\n",
    "print(f'Input shape of the emotion recognition network is n = {emotion_recognition_n}, c={emotion_recognition_c}, h={emotion_recognition_h}, w={emotion_recognition_w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_output_layer = next(iter(emotion_recognition_network.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Prepare a frame and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_recognition_inference (face_frame: np.ndarray):\n",
    "    prepared_frame = pre_process_input_image(face_frame, emotion_recognition_n, emotion_recognition_c, emotion_recognition_h, emotion_recognition_w)\n",
    "    \n",
    "    # Run the inference how you did it early\n",
    "    inference_results = emotion_recognition_network_loaded_on_device.infer({\n",
    "        emotion_recognition_input_layer: prepared_frame\n",
    "    })\n",
    "    \n",
    "    # For understanding what is the result of inference this model, check documentation \n",
    "    # https://docs.openvinotoolkit.org/latest/_models_intel_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html\n",
    "    return inference_results[emotion_recognition_output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Drow boxes and emotions in a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smile_by_index(emotion_inference_result: np.ndarray) -> np.ndarray:\n",
    "    emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "    emotion_index = np.argmax(emotion_inference_result.flatten()) \n",
    "    smile_path = f'./data/{emotions[emotion_index]}.png'\n",
    "    return cv2.imread(smile_path, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_recognition_inference_postpprocess(image, recognized_emotions, xmin, xmax, ymin, ymax):\n",
    "    # Put the title to a frame\n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    \n",
    "    smile = get_smile_by_index(recognized_emotions)\n",
    "    resized_smile = cv2.resize(smile, (w, h))\n",
    "    \n",
    "    alpha_s = resized_smile[:, :, 3] / 255.0\n",
    "    alpha_l = 1.0 - alpha_s\n",
    "    for c in range(0, 3):\n",
    "        image[ymin:ymax, xmin:xmax, c] = (alpha_s * resized_smile[:, :, c] + alpha_l * image[ymin:ymax, xmin:xmax, c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "original_image = load_input_image(input_image_path)\n",
    "original_image_h, original_image_w, *_ = original_image.shape\n",
    "\n",
    "# Resize the input image\n",
    "in_frame = pre_process_input_image(original_image, face_detection_n, face_detection_c, face_detection_h, face_detection_w)\n",
    "\n",
    "# Display the input image\n",
    "print(\"Input image:\")\n",
    "show_images(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_inference_results = face_detection_executable_network.infer(inputs={face_detection_input_blob: in_frame})\n",
    "\n",
    "face_detection_inference_result = face_detection_inference_results[face_detection_output_blob]\n",
    "color = (12.5, 255, 255)\n",
    "        \n",
    "processed_image = original_image\n",
    "# Loop through all possible results\n",
    "for detected_face in face_detection_inference_result[0][0]:\n",
    "    probability = round(detected_face[2] * 100, 1)\n",
    "\n",
    "    # If probability is more than the specified threshold, draw and label the box \n",
    "    if probability > prob_threshold:\n",
    "        # Get coordinates of the box containing the detected object\n",
    "        xmin = int(detected_face[3] * original_image_w)\n",
    "        ymin = int(detected_face[4] * original_image_h)\n",
    "        xmax = int(detected_face[5] * original_image_w)\n",
    "        ymax = int(detected_face[6] * original_image_h)\n",
    "\n",
    "        face = original_image[ymin:ymax, xmin:xmax]\n",
    "                \n",
    "        recognized_emotions = emotion_recognition_inference(face)\n",
    "        emotion_recognition_inference_postpprocess(processed_image, recognized_emotions, xmin, xmax, ymin, ymax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Loop over frames in the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "input_frame_width = int(input_video_stream.get(3))   # float `width`\n",
    "input_frame_height = int(input_video_stream.get(4))  # float `height`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_stream = prapare_out_video_stream(input_video_stream, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    \n",
    "    # Read the next frame from the intput video \n",
    "    finish, original_frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not finish:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    face_detection_frame = pre_process_input_image(original_frame, face_detection_n, face_detection_c, face_detection_h, face_detection_w)\n",
    "    face_detection_inference_results = face_detection_executable_network.infer(inputs={face_detection_input_blob: face_detection_frame})\n",
    "\n",
    "    face_detection_inference_result = face_detection_inference_results[face_detection_output_blob]\n",
    "\n",
    "    # Loop through all possible results\n",
    "    for detected_face in face_detection_inference_result[0][0]:\n",
    "        probability = round(detected_face[2] * 100, 1)\n",
    "\n",
    "        # If probability is more than the specified threshold, draw and label the box \n",
    "        if probability > prob_threshold:\n",
    "            # Get coordinates of the box containing the detected object\n",
    "            xmin = int(detected_face[3] * input_frame_width)\n",
    "            ymin = int(detected_face[4] * input_frame_height)\n",
    "            xmax = int(detected_face[5] * input_frame_width)\n",
    "            ymax = int(detected_face[6] * input_frame_height)\n",
    "\n",
    "            face = original_frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            recognized_emotions = emotion_recognition_inference(face)\n",
    "            emotion_recognition_inference_postpprocess(original_frame, recognized_emotions, xmin, xmax, ymin, ymax)\n",
    "    \n",
    "    output_video_stream.write(original_frame)\n",
    "    \n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the person (Artyom) on the resulting video will be detected with emotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{output_video_path}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
