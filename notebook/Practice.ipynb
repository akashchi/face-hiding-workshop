{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "## 1. [Introduction](#s1)\n",
    "\n",
    "## 2. [OpenVINO™ Overview](#s6)\n",
    "\n",
    "## 3. [OpenVINO™ Deep Learning Workbench](#s7)\n",
    "\n",
    "### 3.1 [DL Workbench Workflow](#s7)\n",
    "\n",
    "## 5. [Practice](#s15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "1. Neural Network\n",
    "\n",
    "Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and an activation function.\n",
    "\n",
    "![](./pictures/neural_network.svg)\n",
    "\n",
    "By Glosser.ca - Own work, [Link](https://commons.wikimedia.org/w/index.php?curid=24913461) CC BY-SA 3.0\n",
    "\n",
    "2. Inference\n",
    "\n",
    "Process of neural network execution: feeding data to the network and getting the results. \n",
    "\n",
    "3. Dataset\n",
    "\n",
    "A dataset is a collection of data that can be treated by a neural network as a single unit for analytic and prediction purposes.\n",
    "\n",
    "4. Optimization\n",
    "\n",
    "To accelerate the inference of deep learning models by applying special methods without model retraining or fine-tuning, like post-training quantization.\n",
    "The process of transforming the models that were trained in the floating-point precision into the models with integer representation with floating/fixed-point quantization operations between the layers.\n",
    "\n",
    "5. Accuracy\n",
    "\n",
    "Measure for how good or bad a neural network solves its task. Accuracy could be represented by different metrics depending on the task. \n",
    "\n",
    "6. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/infer.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenVINO™ toolkit is a comprehensive toolkit for quickly developing applications and solutions that solve a variety of tasks including emulation of human vision, automatic speech recognition, natural language processing, recommendation systems, and many others. Based on latest generations of artificial neural networks, including Convolutional Neural Networks (CNNs), recurrent and attention-based networks, the toolkit extends computer vision and non-vision workloads across Intel® hardware, maximizing performance. It accelerates applications with high-performance, AI and deep learning inference deployed from edge to cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/about_vino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/openvino_toolkit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/additional_tools.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned and optimized OpenVINO models\n",
    "### For various tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pictures/models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Learning Workbench In Depth](./dl_workbench.ipynb)\n",
    "\n",
    "There you can find the DL WB interface examples and a sample workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App - Face Replacer\n",
    "Face detection, Emotion recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (INSERT) Picture before - picture after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "#### Part 0 - Obtain a model\n",
    "0. Go to DL WB\n",
    "1. Find a suitable face-detection model \n",
    "2. Experiment with it, optimize, assess results\n",
    "3. Export\\download the model\n",
    "\n",
    "#### Part 1 - OpenVINO Python API + minimal app\n",
    "\n",
    "Show tutorial_object_detection for OV Python API\n",
    "\n",
    "4. Prerequisites\n",
    "    * copy the model path from DL WB\n",
    "    * sample data (video) is placed in the folder with this notebook\n",
    "5. OpenVINO Python API for work with neural networks\n",
    "6. Image/video pre-processing with OpenCV\n",
    "7. Neural network execution - Inference\n",
    "8. Results processing\n",
    "    * Describe the model and its output so that it is understandable how to post-process\n",
    "9. Have a video with faces replaced\n",
    "\n",
    "#### Part 2 - Enriching/Building upon the app / Adding new functionality\n",
    "\n",
    "9. Prepare another neural network\n",
    "10. Integrate new network in the app\n",
    "\n",
    "OR - give a choice of either following the presenter with deployment of continuing with the emotion recognition\n",
    "\n",
    "#### Part 3 - Deploy the app\n",
    "11. Prepare deployment package\\bundle with model and download it\n",
    "    * Ubuntu - go to DL WB\n",
    "    * non-Ubuntu - supply with os-specific bundles\n",
    "12. Prepare platform\n",
    "    * Copy/download the necessary assets (OpenVINO deployment package, model)\n",
    "    * Prepare environment using setupvars\n",
    "13. Prepare sample\\application and a Telegram bot\n",
    "    * Clone the repository with the template\n",
    "    * Copy your code from the notebook and integrate in the template\n",
    "13. Deploy\n",
    "14. Enjoy\n",
    "\n",
    "\n",
    "#### Demo of the completed bot\\application in case of the out of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Hiding Workshop Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the OpenVINO Inference Engine Python API, see the [official documentation](https://docs.openvinotoolkit.org/latest/ie_python_api/annotated.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to install requirements fo this workshop - packages like `numpy` to work with tensors and IPython to show a video in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of the preparation is import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV for work with a video and images\n",
    "import cv2\n",
    "\n",
    "# Import numpy to work with tensors\n",
    "import numpy as np\n",
    "\n",
    "# to display photos and videos\n",
    "from IPython.display import HTML, display, Image\n",
    "\n",
    "# to work with paths\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenVINO Inference Engine\n",
    "from openvino.inference_engine import get_version, IECore\n",
    "\n",
    "# Show the version of Inference Engine\n",
    "print(f'OpenVINO Inference Engine version is {get_version()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set some constants. Paths to input and result videos and the model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains all data for the workshop\n",
    "WORKSHOP_DATA_PATH = Path('data')\n",
    "\n",
    "# Contains the model for the workshop\n",
    "WORKSHOP_MODEL_PATH = WORKSHOP_DATA_PATH / 'models'\n",
    "\n",
    "# Path to the Inference Engine model\n",
    "FACE_DETECTION_MODEL_PATH_XML = WORKSHOP_MODEL_PATH / 'face-detection-adas-0001.xml'\n",
    "FACE_DETECTION_MODEL_PATH_BIN = WORKSHOP_MODEL_PATH / 'face-detection-adas-0001.bin'\n",
    "\n",
    "# Device which we will use \n",
    "DEVICE = 'CPU'\n",
    "\n",
    "INPUT_IMAGE = str(WORKSHOP_DATA_PATH / 'input_image.jpg')\n",
    "\n",
    "INPUT_VIDEO = str(WORKSHOP_DATA_PATH / 'input.mp4')\n",
    "OUTPUT_VIDEO = str(WORKSHOP_DATA_PATH / 'output.MP4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create an instance of the OpenVINO Inference Engine `IECore` class\n",
    "This class represents an Inference Engine entity \n",
    "and allows you to manipulate plugins using unified interfaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie_core = IECore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read the prepared model\n",
    "\n",
    "To inference a model you need to read the model from the file usinf the instance of `IECore` class.\n",
    "Use the `read_network` method for this. The method has two parameters: \n",
    " 1. path to the .xml file of the model \n",
    " 2. path to the .bin file of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ie_core.read_network(FACE_DETECTION_MODEL_PATH_XML, weights=FACE_DETECTION_MODEL_PATH_BIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get the name of the input layer of the model\n",
    "\n",
    "To infer a model, you need to know input layers of the model.\n",
    "The object `network` has an information about inputs of the network in a property `input_info`,\n",
    "which is a dictionary: key - name of the input layer, volume - representation of the input network.\n",
    "In this case, you need to get the name and the blob of the input .`input_name` should be a string, `input_blob`  should be a `DataPtr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = next(iter(network.input_info))\n",
    "input_blob = network.input_info[input_name].input_data\n",
    "\n",
    "print(f'Name of the network input layer is \"{input_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get shape (dimensions) of the input layer of the network\n",
    "\n",
    "* n - number of batches\n",
    "* c - number of input image channels (usualy 3 - R, G and B) \n",
    "* h - height\n",
    "* w - width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = input_blob.shape\n",
    "\n",
    "print(f'Input shape of the network is {input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_blob_name = next(iter(network.outputs))\n",
    "\n",
    "print(f'Name of the network output layer is \"{output_blob_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Load the network to a device\n",
    "\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`, reterned from `read_network`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_loaded_to_device = ie_core.load_network(network, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Read image from a file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_frame = cv2.imread(INPUT_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Pre-processing of input image \n",
    "The function to prepare readed image for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, input_layer_height, input_layer_width = input_shape\n",
    "\n",
    "# Resize the frame to the network input \n",
    "resized_frame = cv2.resize(input_frame, (input_layer_width, input_layer_height))\n",
    "\n",
    "# Change the data layout from HWC to CHW\n",
    "transposed_frame = resized_frame.transpose((2, 0, 1)) \n",
    "\n",
    "# Reshape the frame to the network input \n",
    "prepared_frame = transposed_frame.reshape(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_result = face_detection_loaded_to_device.infer({\n",
    "        input_name: prepared_frame\n",
    "})\n",
    "\n",
    "inference_result = inference_result[output_blob_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Process inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelize_region(region: np.ndarray) -> np.ndarray:\n",
    "    height, width = region.shape[:2]\n",
    "    pixels_count = 16\n",
    "    temp = cv2.resize(region, (pixels_count, pixels_count), interpolation=cv2.INTER_LINEAR)\n",
    "    return cv2.resize(temp, (width, height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detected_face in inference_result[0][0]:\n",
    "    confidence =  detected_face[2]\n",
    "    if confidence < 0.5:\n",
    "        continue\n",
    "    frame_h, frame_w = input_frame.shape[:2]\n",
    "    \n",
    "    xmin = int(detected_face[3]*frame_w)\n",
    "    ymin = int(detected_face[4]*frame_h)\n",
    "\n",
    "    xmax = int(detected_face[5]*frame_w)\n",
    "    ymax = int(detected_face[6]*frame_h)\n",
    "    \n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    \n",
    "    face = input_frame[ymin:ymax, xmin:xmax]\n",
    "    input_frame[ymin:ymax, xmin:xmax] = pixelize_region(face)\n",
    "    # Get confidence for a discovered object\n",
    "    confidence = round(confidence * 100, 1)\n",
    "    \n",
    "    # Draw a box and a label\n",
    "    color = (0, 255, 0)\n",
    "    \n",
    "    # Create the title of an object\n",
    "    text = f'{confidence}%'\n",
    "\n",
    "    # Put the title to a frame\n",
    "    cv2.putText(input_frame, text, (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(image: np.ndarray):\n",
    "    _, data = cv2.imencode('.jpg', image) \n",
    "    image = Image(data=data)\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(input_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 1. Blure faces on a video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{INPUT_VIDEO}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an IECore Instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie_core = IECore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_network = ie_core.read_network(FACE_DETECTION_MODEL_PATH_XML, weights=FACE_DETECTION_MODEL_PATH_BIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get name of network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_input_name = next(iter(face_detection_network.input_info))\n",
    "face_detection_input_blob = face_detection_network.input_info[face_detection_input_name].input_data\n",
    "\n",
    "print(f'Name of the network input layer is \"{face_detection_input_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get shape (dimensions) of the input layer of the network\n",
    "\n",
    "* n - number of batches\n",
    "* c - number of input image channels (usualy 3 - R, G and B) \n",
    "* h - height\n",
    "* w - width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_input_shape = face_detection_input_blob.shape\n",
    "\n",
    "print(f'Input shape of the network is {face_detection_input_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_output_blob_name = next(iter(face_detection_network.outputs))\n",
    "\n",
    "print(f'Name of the network output layer is \"{face_detection_output_blob_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the network to a device\n",
    "\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`, reterned from `read_network`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_loaded_to_device = ie_core.load_network(face_detection_network, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Open the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(input_frame, network_input_shape):\n",
    "    _, _, input_layer_height, input_layer_width = network_input_shape\n",
    "\n",
    "    # Resize the frame to the network input \n",
    "    resized_frame = cv2.resize(input_frame, (input_layer_width, input_layer_height))\n",
    "\n",
    "    # Change the data layout from HWC to CHW\n",
    "    transposed_frame = resized_frame.transpose((2, 0, 1)) \n",
    "\n",
    "    # Reshape the frame to the network input \n",
    "    reshaped_frame = transposed_frame.reshape(network_input_shape)\n",
    "    \n",
    "    return reshaped_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detection_inference(input_frame: np.ndarray) -> np.ndarray:\n",
    "    feed_dict = {\n",
    "        face_detection_input_name: input_frame\n",
    "    }\n",
    "    \n",
    "    # All is ready for the main thing - inference!\n",
    "    # You have read and loaded the network to the device, prepared input data and now you are ready to infer.\n",
    "    \n",
    "    # To start an inference, call the `infer` function of the `face_detection_loaded_to_device` variable. \n",
    "    # We must set input data (a dictionary).\n",
    "    inference_result = face_detection_loaded_to_device.infer(feed_dict)\n",
    "    \n",
    "    # Great! The `inference_result` variable contains output data after inference of the network.\n",
    "    # `inference_result` is a dictionary, \n",
    "    #  where key is the name of the output name, \n",
    "    #        value is data from the blob.\n",
    "    \n",
    "    return inference_result[face_detection_output_blob_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_face_detection_inference_result(original_frame: np.ndarray, detected_face: np.ndarray):       \n",
    "    \n",
    "    # The inference result contains 7 items\n",
    "    confidence =  detected_face[2]\n",
    "        \n",
    "    if confidence < 0.5:\n",
    "        return\n",
    "    \n",
    "    frame_h, frame_w = original_frame.shape[:2]\n",
    "    \n",
    "    xmin = int(detected_face[3]*frame_w)\n",
    "    ymin = int(detected_face[4]*frame_h)\n",
    "\n",
    "    xmax = int(detected_face[5]*frame_w)\n",
    "    ymax = int(detected_face[6]*frame_h)\n",
    "    \n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    \n",
    "    face = original_frame[ymin:ymax, xmin:xmax]\n",
    "    original_frame[ymin:ymax, xmin:xmax] = pixelize_region(face)\n",
    "    \n",
    "    \n",
    "    # Get confidence for a discovered object\n",
    "    confidence = round(confidence * 100, 1)\n",
    "    \n",
    "    # Draw a box and a label\n",
    "    color = (0, 255, 0)\n",
    "    \n",
    "    # Create the title of an object\n",
    "    text = f'{confidence}%'\n",
    "\n",
    "    # Put the title to a frame\n",
    "    cv2.putText(original_frame, text, (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 2, color, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Prepare output video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prapare_out_video_stream(input_video_stream: cv2.VideoCapture, output_video_file_path: str) -> cv2.VideoWriter:\n",
    "    width  = int(input_video_stream.get(3))\n",
    "    height = int(input_video_stream.get(4))\n",
    "    video_writer = cv2.VideoWriter(output_video_file_path, cv2.VideoWriter_fourcc(*'avc1'), 20, (width, height))\n",
    "    return video_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Output video stream\n",
    "output_video_stream = prapare_out_video_stream(input_video_stream, OUTPUT_VIDEO)\n",
    "\n",
    "# Get input height and width\n",
    "input_frame_width = int(input_video_stream.get(3))   # float `width`\n",
    "input_frame_height = int(input_video_stream.get(4))  # float `height`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Loop over frames in the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    # Read the next frame from the intput video \n",
    "    ret, frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    \n",
    "    # Prepare frame for inference\n",
    "    in_frame = pre_processing(frame, face_detection_input_shape)\n",
    "    \n",
    "    \n",
    "    inference_result = face_detection_inference(in_frame)\n",
    "    \n",
    "    for detected_face in inference_result[0][0]:\n",
    "        post_processing_face_detection_inference_result(frame, detected_face)\n",
    "    \n",
    "    # Write the resulting frame to the output stream\n",
    "    output_video_stream.write(frame)\n",
    "    \n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{OUTPUT_VIDEO}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see boxes in the video? \n",
    "If yes, you did all right!\n",
    "**Good Work!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 16: Practice (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the next step? Often from neural networks build pipelines. It is to use the results of the first neural network as an input for the next neural network. \n",
    "Let's try to build a pipeline from two networks:  first is finds a person on the video and the next to recognize the emotions of this person\n",
    "\n",
    "We have already run the first network. And find the person on the video.\n",
    "The next step is to find a network for emotion recognition.\n",
    "There is a good neural network in the [OpenModelZOO](https://docs.openvinotoolkit.org/2019_R1/_docs_Pre_Trained_Models.html) - [emotions-recognition-retail-0003 network](https://docs.openvinotoolkit.org/2019_R1/_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download emotions-recognition-retail-0003 network\n",
    "Run the Model Downloader eith needed arguments to download the emotions-recognition-retail-0003 network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ~/intel/openvino_2021/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name emotions-recognition-retail-0003 --precision FP16 --output_dir data/model\n",
    "!mv data/model/intel/emotions-recognition-retail-0003/FP16/emotions-recognition-retail-0003.* data/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mode already is in OpenVINO format and you do not need to convert it.\n",
    "\n",
    "After downloading the model you can use it:\n",
    "\n",
    "### Step 2: Read the prepared model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_network = ie_core.read_network('data/models/emotions-recognition-retail-0003.xml', 'data/models/emotions-recognition-retail-0003.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the network to a device\n",
    "\n",
    "Use the instance of `IECore`.\n",
    "The class `IECore` has a special function called `load_network`, which loads a network to a device.\n",
    "This function prepares the network for the first inference on the device \n",
    "and returns an instance of the network prepared for an inference (execution). \n",
    "This function has many parameters, but in this case, you need to know only about two of them:\n",
    "* `network` - instance of `IENetwork`\n",
    "* `device_name` - string, contains a device name to infer a model on: CPU, GPU and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_network_loaded_on_device = ie_core.load_network(emotion_recognition_network, 'CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Open the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_stream = cv2.VideoCapture(INPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create an output video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_stream = prapare_out_video_stream(input_video_stream, OUTPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_input_layer = next(iter(emotion_recognition_network.input_info))\n",
    "emotion_recognition_input_blob = emotion_recognition_network.input_info[emotion_recognition_input_layer].input_data\n",
    "\n",
    "print(f'Input layer of the emotions-recognition-retail-0003 is {emotion_recognition_input_layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_shape = emotion_recognition_input_blob.shape\n",
    "\n",
    "print(f'Input shape of the emotion recognition network is {emotion_recognition_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_recognition_output_layer = next(iter(emotion_recognition_network.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Prepare a frame and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_infer(face_frame: np.ndarray):\n",
    "    prepared_frame = pre_processing(face_frame, emotion_recognition_shape)\n",
    "    # Run the inference how you did it early\n",
    "    inference_results = emotion_recognition_network_loaded_on_device.infer({\n",
    "        emotion_recognition_input_layer: prepared_frame\n",
    "    })\n",
    "    # For understanding what is the result of inference this model, check documentation \n",
    "    # https://docs.openvinotoolkit.org/latest/_models_intel_emotions_recognition_retail_0003_description_emotions_recognition_retail_0003.html\n",
    "    return inference_results[emotion_recognition_output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Drow boxes and emotions in a frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smile_by_index(emotion_inference_result: np.ndarray) -> np.ndarray:\n",
    "    emotions = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
    "    emotion_index = np.argmax(emotion_inference_result.flatten()) \n",
    "    smile_path = f'./data/{emotions[emotion_index]}.png'\n",
    "    return cv2.imread(smile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_recognition_inference_postpprocess(original_frame, detected_face, emotion_result, x_limits, y_limits):\n",
    "    smile = get_smile_by_index(emotion_result)\n",
    "    # Put the title to a frame\n",
    "    w = x_limits[1] - x_limits[0]\n",
    "    h = y_limits[1] - y_limits[0]\n",
    "\n",
    "    resized_smile = cv2.resize(smile, (w, h))\n",
    "    \n",
    "    original_frame[y_limits[0]:y_limits[1], x_limits[0]:x_limits[1]] = resized_smile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Loop over frames in the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input_video_stream.isOpened():\n",
    "    \n",
    "    # Read the next frame from the intput video \n",
    "    ret, original_frame = input_video_stream.read()\n",
    "    # Check if the video is over\n",
    "    if not ret:\n",
    "        # Exit from the loop if the video is over\n",
    "        break \n",
    "    face_detection_frame = pre_processing(original_frame, face_detection_input_shape)\n",
    "    face_detection_inferece_result = face_detection_inference(face_detection_frame)\n",
    "    frame_h, frame_w = original_frame.shape[:2]\n",
    "    for detected_face in face_detection_inferece_result[0][0]:\n",
    "        if detected_face[2] < 0.5:\n",
    "            continue\n",
    "        # Step 13: Get the confidence for a discovered object\n",
    "        xmin = int(detected_face[3]*frame_w)\n",
    "        ymin = int(detected_face[4]*frame_h)\n",
    "\n",
    "        xmax = int(detected_face[5]*frame_w)\n",
    "        ymax = int(detected_face[6]*frame_h)\n",
    "        \n",
    "        emotion_recognition_frame = original_frame[ymin:ymax, xmin:xmax]\n",
    "    \n",
    "        # Get height and width of the frame\n",
    "        emotion_recognition_result = emotion_infer(emotion_recognition_frame)\n",
    "        emotion_recognition_inference_postpprocess(original_frame, detected_face, emotion_recognition_result, (xmin, xmax), (ymin, ymax))\n",
    "        # Write the resulting frame to the output stream\n",
    "    \n",
    "    output_video_stream.write(original_frame)\n",
    "    \n",
    "input_video_stream.release()\n",
    "# Save the resulting video\n",
    "output_video_stream.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the person (Artyom) on the resulting video will be detected with emotion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a source video\n",
    "HTML(f\"\"\"<video width=\"600\" height=\"400\" controls><source src=\"{OUTPUT_VIDEO}\" type=\"video/mp4\"></video>\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
